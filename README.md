````markdown
# ğŸ§ª RAG ChunkTester

**RAG ChunkTester** is a framework for automated testing of Retrieval-Augmented Generation (RAG) systems.  
It allows you to quickly evaluate the impact of different chunking strategies, embedding configurations, retriever parameters, and prompts.

---

## ğŸ“ Project Structure

chunktester/
â”œâ”€â”€ main.py                    # Entry point: run chunking, eval, sweep, summary
â”œâ”€â”€ runners/                   # Execution modules
â”‚   â”œâ”€â”€ chunk_and_embed.py
â”‚   â”œâ”€â”€ eval_runner.py
â”‚   â””â”€â”€ sweep_runner.py
â”œâ”€â”€ utils/                     # Utility helpers
â”‚   â”œâ”€â”€ config_loader.py
â”‚   â”œâ”€â”€ document_loader.py
â”‚   â”œâ”€â”€ chunking.py
â”‚   â”œâ”€â”€ embedding.py
â”‚   â”œâ”€â”€ retriever.py
â”‚   â””â”€â”€ logger.py
â”œâ”€â”€ cleaners/                  # Optional chunk cleaning
â”‚   â””â”€â”€ chunk_cleaner.py
â”œâ”€â”€ scorers/
â”‚   â””â”€â”€ llm_scorer.py          # Response evaluation (+1 / 0 / -1)
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ system_prompt.txt      # Instruction for LLM
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ eval_config.yaml       # Config for a single run
â”‚   â”œâ”€â”€ sweep_grid.yaml        # Grid search parameters
â”‚   â””â”€â”€ settings.py            # Keys and proxy from .env
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ source_docs/           # Documents for chunking
â”‚   â””â”€â”€ queries.jsonl          # Queries + expected keywords
â”œâ”€â”€ embeddings/                # ChromaDB storage (one folder per run)
â”œâ”€â”€ results/                   # Evaluation results (one folder per run)
â”œâ”€â”€ .env                       # API keys and options
â”œâ”€â”€ .gitignore
â””â”€â”€ requirements.txt

---

## ğŸš€ Quick Start

1. **Set up dependencies and create a virtual environment:**

```bash
python -m venv venv
source venv/bin/activate # for Unix
.\venv\Scripts\activate.bat # for Windows
pip install -r requirements.txt
````

2. **Configure `.env` with your keys:**

```env
OPENAI_API_KEY=sk-...
USE_PROXY=false
PROXY_URL=http://localhost:8080
OPENAI_MODEL=gpt-4
```

3. **Place documents into `data/source_docs/`** (`.txt`, `.md`, etc.)

4. **Define queries in `data/queries.jsonl`:**

```json
{"query": "Who is Mr. X?", "expected_keywords": ["director", "analytics"]}
```

5. **Make sure `prompts/system_prompt.txt` contains your instruction**

---

## ğŸ§ª Question Generation - positive/negative/hallucination

```bash
python generate_questions.py
```

---

## ğŸ§ª Single Experiment

```bash
python main.py run_chunk --config configs/eval_config.yaml
python generate_questions.py
python main.py run_eval  --config configs/eval_config.yaml
python main.py summary
python main.py show_fails
python main.py run_eval --config configs/eval_config.yaml --queries data/failed_queries.jsonl
```

---

## ğŸ” Multiple Experiments (sweep)

```bash
python main.py run_sweep --sweep configs/sweep_zip.yaml
python main.py summary
```

---

## ğŸ“Š Evaluation

The scoring metric is based on:

* `+1` â€” all expected keywords found in the answer
* `0` â€” some keywords found
* `-1` â€” no keywords found

Example output:

```text
=== Summary ===
âœ… sweep_a1655fdd: acc=0.87, halluc=0.00 | +1=13, 0=1, -1=1
```

---

## âš ï¸ Important Notes

* `run_chunk` **always creates a new collection** (unique `run_id`)
* `run_eval` uses the `run_id` corresponding to the current config
* if the config does not change â€” you can rerun `run_eval` without `run_chunk`

---

## ğŸ“Œ Planned Improvements

* [ ] Embedding caching by document hash
* [ ] LLM hallucination check (controlled off-context response)
* [ ] Graphical analysis of sweep runs
* [ ] `run_all` wrapper for a full pipeline run

---

## ğŸ¤ Author

This system was developed for internal RAG system testing.
Contact: \[e-mail: [alx1379@gmail.com](mailto:alx1379@gmail.com)]

```
